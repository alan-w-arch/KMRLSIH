{"doc_id": "Annexure-I.pdf", "file_type": "pdf", "file_path": null, "chunk_id": 1, "text_hash": "f8c72aeca7d29599", "summary": "annexure-i team : code terrors nom sexe courriel id mobile no. stream acade (mf) mic an team leader himanshu m himanshusaxena71333 9815759234 bca iii saxena gmail.com an team bhe", "sentences": ["annexure-i team : code terrors name gender email id mobile no.", "stream acade (mf) mic year team leader himanshu m himanshusaxena71333 9815759234 bca iii saxena gmail.com year team bheeshan m bhishansharma3354gm 9872095303 b.sc it iii member sharma ail.com year team himanshu m himanshujha202005gm 9877698593 bca iii member jha ail.com year team shanvi f 0805shanvigmail.com 9064861954 bca iii member year team shwe win m nickytwist70gmail.com 6280741995 b.sc it iii member aung year team akarsh m akarshmi.amgmail.com 8005006549 mca i member mishra year psid: 25080 problem statement: document overload at kochi metro rail limited (kmrl)-an automated solution."]}
{"doc_id": "Annexure-I.pdf", "file_type": "pdf", "file_path": null, "chunk_id": 2, "text_hash": "5181b59cbf671cd9", "summary": "وړاندیز شوی حل: ټول اسناد چې زموږ د اکوسیستم ته ننوځي به د یوځای شوي اپلوډ مرکز کې رامینځته شي، کوم چې د WhatsApp، بریښنالیک، ماماټو، شریک ټکی، او مستقیم اپلوډونو په", "sentences": ["proposed solution: all documents that enter our ecosystem will originate at the unified upload hub, which integrates all sources like whatsapp, email, maximo, sharepoint, and direct uploads.", "the files are instead not stored immediately, but placed on a temporary in-memory representation of the files in the form of json, which can be speedily processed.", "a five-stage ai pipeline is then used to pass the information.", "first, optical character recognition breaks the visual shell of the document setting text free.", "it is polished through the use of natural language processing systems, whichstandardize, normalize and clean it."]}
{"doc_id": "Annexure-I.pdf", "file_type": "pdf", "file_path": null, "chunk_id": 3, "text_hash": "e22db392a14990df", "summary": "エンティティ、サマリー、および暗黙の部門用語は、これらの断片に供給されるトランスフォーマーモデルによって検出されます。パイプラインはフェーズベースの埋め込みインデックスと、構造リポジトリ", "sentences": ["semantically intelligent chunking of refined content into semantically preserving chunks is made.", "entities, summaries, and tacit departmental jargon are detected by transformer models fed on these fragments.", "the pipeline finishes off with the faiss based embedding indexing, and postgresql as a structure repository.", "intelligence goes past storage.", "a hash-based caching layer will provide faster retrieval, whereas a priority classification algorithm will bring out priority directives, which will subsequently result in immediate notifications to stakeholders.", "it has a reactjs front end that is bilingual (englishmalayalam) and has fluid search and secure role-based dashboards."]}
{"doc_id": "Annexure-I.pdf", "file_type": "pdf", "file_path": null, "chunk_id": 4, "text_hash": "6f37d9163d0654ee", "summary": "このようなアーキテクチャは、いくつかの文書管理システムではなく、断片化された多言語のデータストリームを優先順位付け、検索可能、および実行可能な知識に変換するディレクティブのスマートエンジンです。 適切な", "sentences": ["full traceability is ensured by the use of audit logging.", "what arises herewith of such an architecture is not some document management system, but a smart engine of directives, which transforms fragmented, multilingual streams of data into prioritized, searchable, and actionable knowledge.", "it makes sure that the appropriate instruction, translated and categorized, goes to the appropriate individual at the very time that it counts the most.", "flow chartblock diagram3d diagram of the solution:figure 1. sequence diagram of whole projectfigure 2. node to node backend infrastructure"]}
